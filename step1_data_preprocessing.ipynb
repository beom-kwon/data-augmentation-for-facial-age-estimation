{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c16b26-1d54-4c1f-88f1-49069b047d8d",
   "metadata": {},
   "source": [
    "FG-NET dataset by Yanwei Fu<br>\n",
    "I do not own this dataset.<br>\n",
    "This dataset is just used for research purpose.\n",
    "\n",
    "This is the FG-NET data. Obviously, the original FG-NET website does not provide this data any more.<br>\n",
    "I provide them in my homepage. Cory (Ke Chen) gave me this data which is used in his paper\n",
    "\n",
    "[1] Y. Fu et al., ``Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels,`` IEEE TPAMI, 2016.<br>\n",
    "[2] Y. Fu et al., ``Interestingness Prediction by Robust Learning to Rank,`` ECCV, 2014.<br>\n",
    "[3] K. Chen et al., ``Cumulative Attribute Space for Age and Crowd Density Estimation,`` CVPR, 2013.\n",
    "\n",
    "To download all data: http://yanweifu.github.io/FG_NET_data/FGNET.zip\n",
    "\n",
    "The data explanations: (available if you start with http://www.eecs.qmul.ac.uk/~yf300/FG_NET_data/)<br>\n",
    "./images folder: all human face images.<br>\n",
    "The groundtruth is used to name each image.<br>\n",
    "For example, 078A11.JPG, means that this is the No.'78' person's image when he/she was 11 years old.<br>\n",
    "'A' is short for Age.\n",
    "\n",
    "./points folder: this is the 68 manual annotated points for each image in ./images folder.<br>\n",
    "The annotated data is of much higher quality than another dataset e.g. MORPH (saved in /export/beware/thumper/yf300/Age_estimation_org_data_backup/ageEstimation/MOPRH).<br>\n",
    "However, MORPH is much bigger dataset than FG-NET.\n",
    "\n",
    "./feature_generation_tools: this is the tool to generate the features.<br>\n",
    "./feature_generation_tools/how-to-use-it: tutorial of how to use the tools.<br>\n",
    "./age50_10_round.mat is the 10 rounds of data used in my work [1].<br>\n",
    "Normally, you should firstly split the training/testing data by yourself.<br>\n",
    "And generate the low-level feature for training/testing data respectively.<br>\n",
    "For each split, the training/testing features are not the same.<br>\n",
    "Because the process of generating training features is also needed to refer the annotations of testing features.\n",
    "\n",
    "There is another very good tutorial and matlab labelling tool for AAM/ASM.<br>\n",
    "You can download it from: http://yanweifu.github.io/FG_NET_data/AAM_verygood.rar<br>\n",
    "But some of them were written in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7084b6-79a5-42eb-a6e9-ae997b35ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def read_pts(fname):\n",
    "    f = open(fname)\n",
    "    lines = f.readlines()\n",
    "    n_points = int(lines[1].split(':')[1].strip())\n",
    "    \n",
    "    data_lines = lines[3:(n_points+3):1]\n",
    "    points = []\n",
    "    for line in data_lines:\n",
    "        coords = line.strip().split()\n",
    "        points.append(list(map(float, coords)))\n",
    "        \n",
    "    f.close()    \n",
    "    return np.array(points)\n",
    "\n",
    "\n",
    "def FGNET_DataLoader():\n",
    "    cwd = os.getcwd()\n",
    "    path = [cwd + \"\\\\FGNET\\\\images\\\\\", cwd + \"\\\\FGNET\\\\points\\\\\"]\n",
    "    \n",
    "    file_list_jpg = [file for file in os.listdir(path[0]) if file.endswith(\".JPG\")]\n",
    "    file_list_pts = [file for file in os.listdir(path[1]) if file.endswith(\".pts\")]\n",
    "\n",
    "    face_list, age_list, pid_list = [], [], []\n",
    "    for file_name_jpg, file_name_pts in zip(file_list_jpg, file_list_pts):\n",
    "        img = cv2.imread(os.path.join(path[0], file_name_jpg), cv2.IMREAD_GRAYSCALE)  # cv2.IMREAD_COLOR\n",
    "\n",
    "        ### (Start) Face Alignment\n",
    "        landmarks = read_pts(os.path.join(path[1], file_name_pts))\n",
    "        # 31: Left Pupil\n",
    "        # 36: Right Pupil\n",
    "        pupils = landmarks[[31, 36], :]\n",
    "        \n",
    "        delta_x, delta_y = pupils[1] - pupils[0]\n",
    "        angle = np.arctan(delta_y / delta_x) * 180 / np.pi\n",
    "        \n",
    "        h, w = img.shape\n",
    "        (cx, cy) = (h // 2, w // 2)\n",
    "\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center=(cx, cy), angle=angle, scale=1)\n",
    "        img = cv2.warpAffine(img, rotation_matrix, dsize=(img.shape[1], img.shape[0]))\n",
    "        ### (End) Face Alignment\n",
    "\n",
    "        ### (Start) Face Cropping\n",
    "        landmarks = np.transpose(np.dot(rotation_matrix[:, 0:2], np.transpose(landmarks))) + np.transpose(rotation_matrix[:, 2])\n",
    "        \n",
    "        x_min, y_min = np.min(landmarks[:, 0]).astype(int), np.min(landmarks[:, 1]).astype(int)\n",
    "        x_max, y_max = np.max(landmarks[:, 0]).astype(int), np.max(landmarks[:, 1]).astype(int)\n",
    "\n",
    "        height = y_max - y_min + 1\n",
    "        width = x_max - x_min + 1\n",
    "        if height > width:\n",
    "            diff = height - width\n",
    "            pad = diff // 2\n",
    "            img = img[y_min:(y_max + 1), (x_min - pad):(x_max + 1 + (diff - pad))]\n",
    "        elif height < width:\n",
    "            diff = width - height\n",
    "            pad = diff // 2\n",
    "            img = img[(y_min - pad):(y_max + 1 + (diff - pad)), x_min:(x_max + 1)]\n",
    "        else:\n",
    "            img = img[y_min:(y_max + 1), x_min:(x_max + 1)]\n",
    "\n",
    "        ### (End) Face Cropping\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img = clahe.apply(img)\n",
    "        \n",
    "        img = cv2.resize(src=img, dsize=(48, 48), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        face_list.append(img)                      # Face Image\n",
    "        age_list.append(int(file_name_jpg[4:6]))   # Age\n",
    "        pid_list.append(int(file_name_jpg[0:3]))   # Person ID for Leave One Person Out(LOPO)\n",
    "    \n",
    "    return face_list, age_list, pid_list\n",
    "\n",
    "\n",
    "def DrawImages(img, ratio=1):\n",
    "    n = len(img)\n",
    "    rows = int(np.ceil(n/10))\n",
    "    if rows < 2:\n",
    "        cols = n\n",
    "    else:\n",
    "        cols = 10\n",
    "    \n",
    "    plt.figure(figsize=(cols * ratio, rows * ratio))\n",
    "    for j in range(rows):\n",
    "        for k in range(cols):\n",
    "            if j * 10 + k < n:\n",
    "                plt.subplot(rows, cols, j * 10 + k + 1)\n",
    "                plt.imshow(img[j * 10 + k], cmap=\"gray\")\n",
    "                plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def VisualizeCounts(arr, xaxis_name):\n",
    "    pos, height = np.unique(arr, return_counts=True)\n",
    "    plt.figure()\n",
    "    plt.bar(pos, height, color=\"royalblue\")\n",
    "    plt.xlabel(xaxis_name)\n",
    "    plt.ylabel(\"The Number of Samples\")  # Count\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def VisualizeScatter(arr1, arr2):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.scatter(arr1, arr2, s=0.8, c=\"blue\")\n",
    "    plt.xlabel(\"Person ID\")\n",
    "    plt.ylabel(\"Age\")\n",
    "    plt.grid(True)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eeda19-3a66-44a0-9ace-24dd7eb966c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists(os.getcwd() + \"\\\\FGNET\\\\npz\\\\\")):\n",
    "    os.mkdir(os.getcwd() + \"\\\\FGNET\\\\npz\")\n",
    "\n",
    "x, y, pid = FGNET_DataLoader()\n",
    "np.savez_compressed(file=os.getcwd() + \"\\\\FGNET\\\\npz\\\\fgnet.npz\", x=x, y=y, pid=pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca4f9d-6ad9-4a8f-8efb-ffe5a053f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(file=os.getcwd() + \"\\\\FGNET\\\\npz\\\\fgnet.npz\")\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "pid = data[\"pid\"]\n",
    "\n",
    "VisualizeCounts(y, \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab9bc3-f2ea-4ea5-870c-d5c52368055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeCounts(pid, \"Person ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86820fb-99f0-4ae7-9d72-57cfe58c94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeScatter(pid, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155d721-f69a-4891-a0e9-a365cb24ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawImages(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
