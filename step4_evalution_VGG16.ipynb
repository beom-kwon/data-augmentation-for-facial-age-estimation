{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce60f4-2721-47bf-a566-b61509e4a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import keras\n",
    "from keras import Model, Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Dense, Flatten, Resizing, Dropout, Lambda\n",
    "\n",
    "\n",
    "def VisualizeLossCurve(h):\n",
    "    plt.figure()\n",
    "    plt.plot(h.history[\"loss\"], label=\"Training\", color=\"red\")\n",
    "    plt.plot(h.history[\"val_loss\"], label=\"Validation\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ae3af-b604-4aa1-8889-7f038dd4bc66",
   "metadata": {},
   "source": [
    "# **1. VGG16 + Random Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9693e4-4d0e-4637-b70d-bf810d2cadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "idx_tc = 0  # index of test cases [0, 1]; 0 for baseline and 1 for Proposed\n",
    "\n",
    "n_iter = 82\n",
    "train_results = []\n",
    "test_results = []\n",
    "for j in range(0, n_iter, 1):\n",
    "    print(\"Fold: %2d out of %d\" % (j + 1, n_iter), end=', ')\n",
    "    \n",
    "    if idx_tc == 0:  # Baseline Performance\n",
    "        data = np.load(file=os.getcwd() + \"/FGNET/npz/fgnet_lopo%02d.npz\" % (j + 1))\n",
    "    elif idx_tc == 1:  # Proposed Data Augmentation\n",
    "        data = np.load(file=os.getcwd() + \"/FGNET/npz/fgnet_lopo_da%02d.npz\" % (j + 1))\n",
    "        \n",
    "    \n",
    "    x_train, y_train = data[\"x_train\"].astype(\"float32\"), data[\"y_train\"]\n",
    "    x_test, y_test = data[\"x_test\"].astype(\"float32\"), data[\"y_test\"]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    x_train = x_train.reshape(len(x_train), 48, 48, 1)\n",
    "    x_val = x_val.reshape(len(x_val), 48, 48, 1)\n",
    "    x_test = x_test.reshape(len(x_test), 48, 48, 1)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    \n",
    "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    \n",
    "    # VGG16 Architecture\n",
    "    inputs = Input(shape=(48, 48, 1))  # Input Layer\n",
    "    x = Resizing(height=224, width=224)(inputs)\n",
    "    x = Lambda(tf.image.grayscale_to_rgb, output_shape=(224, 224, 3))(x)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    base_model = VGG16(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
    "    \n",
    "    x = base_model(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=256, activation=\"relu\")(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    outputs = Dense(units=1)(x)  # Output Layer\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    # model.summary()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(), \n",
    "                  loss=keras.losses.MeanSquaredError(), \n",
    "                  metrics=[\"mae\"])\n",
    "    \n",
    "    modelpath = os.getcwd() + \"\\\\VGG16_Radom.keras\"  # Random Initialization\n",
    "    mc = keras.callbacks.ModelCheckpoint(filepath=modelpath, monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "    es = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    \n",
    "    history = model.fit(train_ds, epochs=EPOCHS, verbose=0, callbacks=[es], validation_data=val_ds)\n",
    "    # VisualizeLossCurve(history)\n",
    "    \n",
    "    scores = model.evaluate(train_ds, verbose=0)\n",
    "    train_results.append(scores)\n",
    "    print(\"Training Set: MAE=%.4f, MSE=%.4f\" % (scores[1], scores[0]), end=' / ')\n",
    "    \n",
    "    scores = model.evaluate(test_ds, verbose=0)\n",
    "    test_results.append(scores)\n",
    "    print(\"Test Set: MAE=%.4f, MSE=%.4f\" % (scores[1], scores[0]))\n",
    "    \n",
    "    y_pred = model.predict(test_ds, verbose=0)\n",
    "\n",
    "\n",
    "train_mean = np.array(train_results).mean(axis=0)\n",
    "test_mean = np.array(test_results).mean(axis=0)\n",
    "\n",
    "print(\"\\nAverage Performance\")\n",
    "print(\"Training Set: MAE=%.4f, MSE=%.4f\" % (train_mean[1], train_mean[0]))\n",
    "print(\"Test Set: MAE=%.4f, MSE=%.4f\" % (test_mean[1], test_mean[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488dd77d-5a99-49e9-b516-c5094910e16e",
   "metadata": {},
   "source": [
    "# **2. VGG16 + Pre-training on ImageNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a13e2-ba8f-42eb-8adf-be8f566ab273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "idx_tc = 0  # index of test cases [0, 1]; 0 for baseline and 1 for Proposed\n",
    "\n",
    "n_iter = 82\n",
    "train_results = []\n",
    "test_results = []\n",
    "for j in range(0, n_iter, 1):\n",
    "    print(\"Fold: %2d out of %d\" % (j + 1, n_iter), end=', ')\n",
    "    \n",
    "    if idx_tc == 0:  # Baseline Performance\n",
    "        data = np.load(file=os.getcwd() + \"/FGNET/npz/fgnet_lopo%02d.npz\" % (j + 1))\n",
    "    elif idx_tc == 1:  # Proposed Data Augmentation\n",
    "        data = np.load(file=os.getcwd() + \"/FGNET/npz/fgnet_lopo_da%02d.npz\" % (j + 1))\n",
    "\n",
    "    \n",
    "    x_train, y_train = data[\"x_train\"].astype(\"float32\"), data[\"y_train\"]\n",
    "    x_test, y_test = data[\"x_test\"].astype(\"float32\"), data[\"y_test\"]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    x_train = x_train.reshape(len(x_train), 48, 48, 1)\n",
    "    x_val = x_val.reshape(len(x_val), 48, 48, 1)\n",
    "    x_test = x_test.reshape(len(x_test), 48, 48, 1)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    \n",
    "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE).cache()\n",
    "    \n",
    "    # VGG16 Architecture\n",
    "    inputs = Input(shape=(48, 48, 1))  # Input Layer\n",
    "    x = Resizing(height=224, width=224)(inputs)\n",
    "    x = Lambda(tf.image.grayscale_to_rgb, output_shape=(224, 224, 3))(x)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    base_model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(x, training=False)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=256, activation=\"relu\")(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    outputs = Dense(units=1)(x)  # Output Layer\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    # model.summary()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(), \n",
    "                  loss=keras.losses.MeanSquaredError(), \n",
    "                  metrics=[\"mae\"])\n",
    "    \n",
    "    modelpath = os.getcwd() + \"\\\\VGG16_ImageNet.keras\"  # Pre-training on ImageNet\n",
    "    mc = keras.callbacks.ModelCheckpoint(filepath=modelpath, monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "    es = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    \n",
    "    history = model.fit(train_ds, epochs=EPOCHS, verbose=0, callbacks=[es], validation_data=val_ds)\n",
    "    # VisualizeLossCurve(history)\n",
    "    \n",
    "    scores = model.evaluate(train_ds, verbose=0)\n",
    "    train_results.append(scores)\n",
    "    print(\"Training Set: MAE=%.4f, MSE=%.4f\" % (scores[1], scores[0]), end=' / ')\n",
    "    \n",
    "    scores = model.evaluate(test_ds, verbose=0)\n",
    "    test_results.append(scores)\n",
    "    print(\"Test Set: MAE=%.4f, MSE=%.4f\" % (scores[1], scores[0]))\n",
    "    \n",
    "    y_pred = model.predict(test_ds, verbose=0)\n",
    "\n",
    "\n",
    "train_mean = np.array(train_results).mean(axis=0)\n",
    "test_mean = np.array(test_results).mean(axis=0)\n",
    "\n",
    "print(\"\\nAverage Performance\")\n",
    "print(\"Training Set: MAE=%.4f, MSE=%.4f\" % (train_mean[1], train_mean[0]))\n",
    "print(\"Test Set: MAE=%.4f, MSE=%.4f\" % (test_mean[1], test_mean[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
